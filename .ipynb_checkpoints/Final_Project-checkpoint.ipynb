{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware: Works only on Kaggle\n",
    "\n",
    "Hangs on laptop. Needs more RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "import gc\n",
    "import random as python_random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras seeding to produce reproduciable results\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gc' (built-in)>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype in [\"int32\", \"int64\"]]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float16)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int16)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in tqdm(lags):\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Months choosen for lag\n",
    "def lag_features(df, lags, ref_col, index_col, col_to_lag):\n",
    "    '''\n",
    "    ref_col: Reference column relative to which lag is measured\n",
    "    lags: List of lag period\n",
    "    index_col: Columns which will be static\n",
    "    col_to lag: Columns on which lag has to be generated\n",
    "    \n",
    "    The function first makes a dataframe with index column and the columns which are\n",
    "    to be lagged. Then it shifts the reference column forward to the lag period.\n",
    "    Other column in the index remains the same. Then only new columns names are generated\n",
    "    which represents the lag period but the values are same as original.\n",
    "    \n",
    "    Thus the reference column is shifted forward, the column names for lag column are suffixed\n",
    "    with lag period and all other elements remain same. If original value of refernce column is \n",
    "    m, lag required is n. Then new reference value is m+n. The rename column is suffixed '_n'.\n",
    "    The lag column value now shows value of n period back from the refernce perion.\n",
    "    \n",
    "    '''\n",
    "    for month_shift in tqdm(lags):\n",
    "        # Create a temp df\n",
    "        lag_shift = df[index_cols + col_to_lag].copy()\n",
    "\n",
    "        # Shift Month column value\n",
    "        lag_shift[ref_col] = lag_shift[ref_col] + month_shift\n",
    "\n",
    "        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in col_to_lag else x\n",
    "        lag_shift = lag_shift.rename(columns=foo)\n",
    "\n",
    "        # Iteratively add all the lagging months\n",
    "        df = pd.merge(df, lag_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "    \n",
    "    del lag_shift\n",
    "    return df\n",
    "\n",
    "\n",
    "def price_trend(df,col1,col2,col3):\n",
    "    \n",
    "    gb = (df.groupby([col1,col2])[col3].sum()\n",
    "          .reset_index()\n",
    "          .rename(columns = {col3:'avg_price'}))\n",
    "    \n",
    "    gb['shifted_avg_price'] = gb.groupby([col2])['avg_price'].shift(1)\n",
    "    \n",
    "    gb.drop('avg_price', axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.merge(df,gb,on=[col1,col2])\n",
    "    df['price_trend'] = df[col3] - df['shifted_avg_price']\n",
    "    df.drop(['shifted_avg_price'], axis=1, inplace=True)\n",
    "    \n",
    "    del gb\n",
    "    \n",
    "    return df.fillna(0)\n",
    "\n",
    "\n",
    "# Measures the delta changes in a column with respect to other columns.\n",
    "def delta_change(df1, df2, gb_cols, pivot_col, agg_col, agg_func='sum', drop_col=True):\n",
    "    '''\n",
    "    gb_cols are the index columns w.r.t which the changes will be calculated\n",
    "    pivot_col is the column subset of the index column relative to which the\n",
    "    change is computed\n",
    "    agg_col is the column which is monitored for change\n",
    "    agg_func is the function (sum, mean...) which will be used for aggregation\n",
    "    drop col is the boolean to know whether the aggregator column and the averaging\n",
    "    column to be retained or not.\n",
    "    \n",
    "    The function first aggregates a column on the groupby statement of index columns of the\n",
    "    training dataframe. Then the aggregrated column average is computed for the pivot column. \n",
    "    This mean is then subtracted from the aggregated column to get the delta column for the \n",
    "    new dataframe being built.\n",
    "    '''\n",
    "    \n",
    "    aggregated_col = '_'.join(gb_cols) + '_agg'\n",
    "    averaged_col = '_'.join(gb_cols) + '_avg_agg'\n",
    "    delta_col = 'delta_' + '_'.join(gb_cols) + '_' + agg_col\n",
    "    \n",
    "    gb = (df1.groupby(gb_cols)\n",
    "                 .agg({agg_col: agg_func})\n",
    "                 .rename(columns = {agg_col:aggregated_col})\n",
    "                 .reset_index())\n",
    "\n",
    "    df2 = pd.merge(df2, gb, how='left', on=gb_cols)\n",
    "\n",
    "    gb = (gb.groupby(pivot_col)\n",
    "                 .agg({aggregated_col: 'mean'})\n",
    "                 .rename(columns = {aggregated_col:averaged_col})\n",
    "                 .reset_index())\n",
    "\n",
    "    df2 = pd.merge(df2, gb, how='left', on=[pivot_col])\n",
    "    df2[delta_col] = ((df2[aggregated_col] - \n",
    "                                  df2[averaged_col]) / df2[averaged_col])\n",
    "    \n",
    "    if drop_col:\n",
    "        df2.drop([aggregated_col,averaged_col], axis=1,inplace=True)\n",
    "    \n",
    "    del gb\n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "def me_gb(df1, df2, gb_cols, agg_col, agg_func='sum', rename_specific = False, new_name = ''):\n",
    "    '''\n",
    "    gb_cols are the index columns w.r.t which the changes will be calculated\n",
    "    \n",
    "    agg_col is the column which is monitored for change\n",
    "    agg_func is the function (sum, mean...) which will be used for aggregation\n",
    "    rename_specific is the boolean to know whether the aggregator column  has to be\n",
    "    renamed\n",
    "    new_name is the new name that is to be given if changed\n",
    "    \n",
    "    The function first aggregates a column on the groupby statement of index columns of the\n",
    "    training dataframe. Then it is merged to the new dataframe that is being built.\n",
    "    \n",
    "    '''\n",
    "    # Get first 4 characters\n",
    "    list_derived = []\n",
    "    for i in gb_cols:\n",
    "        list_derived.append(i[:4])\n",
    "    \n",
    "    if rename_specific:\n",
    "        aggregated_col = new_name\n",
    "    else:\n",
    "        aggregated_col = '_'.join(list_derived) + '_' + agg_col[-3:]\n",
    "        \n",
    "    \n",
    "    \n",
    "    gb = (df1.groupby(gb_cols)\n",
    "                 .agg({agg_col: agg_func})\n",
    "                 .rename(columns = {agg_col:aggregated_col})\n",
    "                 .reset_index())\n",
    "\n",
    "    df2 = pd.merge(df2, gb, how='left', on=gb_cols)\n",
    "\n",
    "    del gb\n",
    "    \n",
    "    return (aggregated_col, df2.fillna(0))\n",
    "\n",
    "gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into variable\n",
    "\n",
    "## For Kaggle\n",
    "\n",
    "train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n",
    "items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\n",
    "category = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n",
    "shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n",
    "test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n",
    "\n",
    "# For laptop\n",
    "\n",
    "# train = pd.read_csv('./final_project_data/sales_train.csv')\n",
    "# items = pd.read_csv('./final_project_data/items.csv')\n",
    "# category = pd.read_csv('./final_project_data/item_categories.csv')\n",
    "# shops = pd.read_csv('./final_project_data/shops.csv')\n",
    "# test = pd.read_csv('./final_project_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " before duplicate drop:(214200, 3)  after duplicate drop:(214200, 3)\n"
     ]
    }
   ],
   "source": [
    "# Copy the original dataset to temp \n",
    "sales = train.copy()\n",
    "sales_test = test.copy()\n",
    "\n",
    "sales_index = sales_test['ID']\n",
    "sales_test.drop_duplicates()\n",
    "print(f' before duplicate drop:{test.shape}  after duplicate drop:{sales_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['split'] = sales['date'].str.split('.')\n",
    "sales['year'] = sales['split'].map(lambda x: int(x[2][-2:]))\n",
    "sales['year'] = sales['year'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.drop(['date','split'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_33 = sales.loc[sales['date_block_num'] == 33]\n",
    "item_sales_last_month = sales_33.groupby(['shop_id','item_id'])['item_cnt_day'].sum().reset_index()\n",
    "item_price_last_month = sales_33.groupby(['shop_id','item_id'])['item_price'].mean().reset_index()\n",
    "# sales_last_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 6)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_test['year'] = 2015\n",
    "sales_test['date_block_num'] = 34\n",
    "sales_test = sales_test.merge(item_sales_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\n",
    "sales_test = sales_test.merge(item_price_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\n",
    "\n",
    "sales_test.drop('ID', axis=1, inplace=True)\n",
    "# sales_test.head()\n",
    "sales_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2935849, 6)\n",
      "(3150049, 6)\n"
     ]
    }
   ],
   "source": [
    "print(sales.shape)\n",
    "sales = sales.append(sales_test, ignore_index=True, sort=False)\n",
    "print(sales.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3150044</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>18454</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150045</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>16188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150046</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>15757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150047</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>19648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150048</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_block_num  shop_id  item_id  item_price  item_cnt_day  year  \\\n",
       "3150044              34       45    18454        99.0           1.0  2015   \n",
       "3150045              34       45    16188         0.0           0.0  2015   \n",
       "3150046              34       45    15757         0.0           0.0  2015   \n",
       "3150047              34       45    19648         0.0           0.0  2015   \n",
       "3150048              34       45      969         0.0           0.0  2015   \n",
       "\n",
       "         month  \n",
       "3150044     10  \n",
       "3150045     10  \n",
       "3150046     10  \n",
       "3150047     10  \n",
       "3150048     10  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales['month'] = sales['date_block_num'] % 12\n",
    "sales.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631921</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719169</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531518</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267562</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323423</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_block_num  month  year\n",
       "631921                6      6    13\n",
       "2719169              30      6    15\n",
       "531518                5      5    13\n",
       "1267562              12      0    14\n",
       "2323423              24      0    15"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_year = sales[['date_block_num','month','year']].drop_duplicates()\n",
    "month_year['date_block_num'].astype(np.int8)\n",
    "month_year['month'].astype(np.int8)\n",
    "month_year['year'].astype(np.int16)\n",
    "month_year.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "#shops\n",
    "shop_city = shops[['shop_id','city_code']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "category['split'] = category['item_category_name'].str.split('-')\n",
    "category['type'] = category['split'].map(lambda x: x[0].strip())\n",
    "category['type_code'] = LabelEncoder().fit_transform(category['type'])\n",
    "\n",
    "category['subtype'] = category['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "category['subtype_code'] = LabelEncoder().fit_transform(category['subtype'])\n",
    "category = category[['item_category_id','type_code', 'subtype_code']]\n",
    "\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.merge(sales, item_category_mapping, how='left', on='item_id')\n",
    "\n",
    "sales = pd.merge(sales, category, on=['item_category_id'], how='left')\n",
    "\n",
    "sales = pd.merge(sales, shop_city, on=['shop_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['revenue'] = sales['item_cnt_day'] * sales['item_price']\n",
    "\n",
    "sales['revenue'] = sales['revenue'] / sales['revenue'].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a feature matrix\n",
    "\n",
    "* itertools.product(*iterables):\n",
    "\n",
    "It returns the cartesian product of all the itrable provided as the argument. For example, product(arr1, arr2, arr3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['date_block_num', 'shop_id', 'item_id']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[[block_num], cur_shops, cur_items ])),dtype='int16'))\n",
    "\n",
    "    # Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join it to the grid\n",
    "# grid is formed by all combination of unique shop,ite,month hence is many more rows,\n",
    "\n",
    "all_data = pd.merge(grid, item_category_mapping, how='left', on='item_id')\n",
    "\n",
    "all_data = pd.merge(all_data, category, on=['item_category_id'], how='left')\n",
    "\n",
    "all_data = pd.merge(all_data, shop_city, on=['shop_id'], how='left')\n",
    "\n",
    "all_data = pd.merge(all_data, month_year, on=['date_block_num'], how='left')\n",
    "\n",
    "\n",
    "del item_category_mapping, category, shop_city, month_year\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id', 'item_id'], \n",
    "                            'item_cnt_day', agg_func='sum', rename_specific = True, new_name = 'target'))\n",
    "\n",
    "list_1_lag = [ret_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id'], \n",
    "                            'item_cnt_day', agg_func='sum', rename_specific = True, new_name = 'target_shop'))\n",
    "\n",
    "list_2_lag = [ret_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'item_id'], \n",
    "                            'item_cnt_day', agg_func='sum', rename_specific = True, new_name = 'target_item'))\n",
    "\n",
    "list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id', 'item_id'], \n",
    "                            'item_price', agg_func='mean', rename_specific = True, new_name = 'item_price'))\n",
    "\n",
    "list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'item_category_id'], \n",
    "                            'item_cnt_day', agg_func='mean', rename_specific = True, new_name = 'date_item_cat_cnt'))\n",
    "\n",
    "list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_day', \n",
    "                            agg_func='mean', rename_specific = True, new_name = 'date_shop_item_cat_cnt'))\n",
    "\n",
    "list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'type_code'], 'item_cnt_day', \n",
    "                           agg_func='mean', rename_specific = True, new_name = 'date_type_item_cnt'))\n",
    "\n",
    "list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'subtype_code'], 'item_cnt_day', \n",
    "                           agg_func='mean', rename_specific = True, new_name = 'date_subtype_item_cnt'))\n",
    "\n",
    "list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'city_code'], 'item_cnt_day', \n",
    "                           agg_func='mean', rename_specific = True, new_name = 'date_city_item_cnt'))\n",
    "\n",
    "# list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['revenue'] = all_data['target'] * all_data['item_price']\n",
    "\n",
    "all_data['revenue'] = all_data['revenue'] / all_data['revenue'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb = (sales.groupby(['date_block_num','revenue'])\n",
    "#                  .agg({'item_cnt_day': 'mean'})\n",
    "#                  .rename(columns = {'item_cnt_day':'date_revenue_item_cnt'})\n",
    "#                  .reset_index())\n",
    "# all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','revenue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'revenue'], 'item_cnt_day', \n",
    "                           agg_func='mean', rename_specific = True, new_name = 'date_revenue_item_cnt'))\n",
    "\n",
    "# list_2_lag.append(ret_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = delta_change(sales, all_data, ['date_block_num', 'shop_id'], 'shop_id', 'item_cnt_day')\n",
    "all_data = delta_change(sales, all_data, ['date_block_num', 'shop_id'], 'shop_id', 'revenue', drop_col=False)\n",
    "all_data = delta_change(sales, all_data, ['month', 'shop_id'], 'shop_id', 'item_cnt_day', 'mean', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = price_trend(all_data,'date_block_num','shop_id','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = price_trend(all_data,'month','shop_id','revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-60c2c26de7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Downcast dtypes from 64 to 32 bit to save memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msales_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gb' is not defined"
     ]
    }
   ],
   "source": [
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "del grid\n",
    "del sales, sales_test\n",
    "gc.collect();\n",
    "\n",
    "all_data = downcast_dtypes(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag feature\n",
    "\n",
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = lag_features(all_data,[1,2, 3, 4], 'date_block_num',index_cols,['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = (lag_features(all_data,[1,2], 'date_block_num',index_cols,\n",
    "             ['target_shop','target_item','date_item_cat_cnt','item_price',\n",
    "                  'date_shop_item_cat_cnt', 'date_type_item_cnt', \n",
    "                  'date_subtype_item_cnt']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, \n",
    "                        ['date_revenue_item_cnt']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['target_shop','target_item','date_item_cat_cnt','date_shop_item_cat_cnt', 'date_type_item_cnt', \n",
    "                'date_subtype_item_cnt','date_revenue_item_cnt',\n",
    "               'revenue','item_price','month', 'year'], axis=1,inplace=True)\n",
    "\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, ['date_city_item_cnt']))\n",
    "\n",
    "# all_data.drop('revenue', axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Category for each item\n",
    "\n",
    "\n",
    "all_data = all_data.fillna(0)\n",
    "all_data = downcast_dtypes(all_data)\n",
    "all_data['shop_id'] = all_data['shop_id'].astype(np.int8)\n",
    "all_data['item_id'] = all_data['item_id'].astype(np.int8)\n",
    "all_data['city_code'] = all_data['city_code'].astype(np.int8)\n",
    "all_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "\n",
    "to_drop_cols = ['target','item_category_id','date_block_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.drop('shop_item', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in ```all_data variable```. Take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/ validation/test split\n",
    "\n",
    "34th month data is the test set. 32nd and 33rd data will be taken as validation split and rest as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "\n",
    "X_train = all_data.loc[(dates <  32)].drop(to_drop_cols, axis=1)\n",
    "X_val = all_data.loc[(dates ==  33) | (dates ==  32)].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == 34].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[(dates <  32), 'target'].values\n",
    "y_val =  all_data.loc[((dates ==  33) | (dates ==  32)), 'target'].values\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu',input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "       layers.Dense(1)\n",
    "      ])\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "    optimizer='rmsprop'\n",
    "    \n",
    "    model.compile(loss=root_mean_squared_error,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model = build_model()\n",
    "\n",
    "# EPOCHS = 10\n",
    "\n",
    "# history = keras_model.fit(\n",
    "#   X_train, y_train,\n",
    "#   epochs=EPOCHS,\n",
    "#     batch_size=100,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     #callbacks=[callbacks],\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# # plt.ylim(bottom=10, top=12)\n",
    "# # plt.xlim(left=1100, right = 1200)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing of test data, fit model\n",
    "# preds_test = keras_model.predict(X_test)\n",
    "\n",
    "# # The prediction is of type numpy.ndarray\n",
    "# preds_list = preds_test.tolist()\n",
    "\n",
    "# # Extract the prediction and put it in a list\n",
    "# prediction = []\n",
    "# for item in preds_list:\n",
    "#     prediction.append(item[0])\n",
    "\n",
    "# prediction = np.clip(prediction, 0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    max_depth=8,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 3)\n",
    "\n",
    "time.time() -start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb_model.predict(X_test).clip(0, 20)\n",
    "\n",
    "prediction = (np.clip(preds, 0, 20)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'ID': sales_index,\n",
    "                       'item_cnt_month': prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv('XbgModel.csv', index=False)\n",
    "# output.to_csv('KerasModel.csv', index=False)\n",
    "output.to_csv('XGBModel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax, importance_type='weight')\n",
    "\n",
    "plot_features(xgb_model, (10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gain = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='gain'), index=['gain'])\n",
    "df_weight = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='weight'), index=['weight'])\n",
    "df_cover = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='cover'), index=['cover'])\n",
    "# df_total_gain = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='total_gain'), index=['total_gain'])\n",
    "# df_total_cover = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='total_cover'), index=['total_cover'])\n",
    "\n",
    "df_importance = df_gain.append(df_weight, ignore_index=False, sort=False)\n",
    "df_importance = df_importance.append(df_cover, ignore_index=False, sort=False)\n",
    "# df_importance = df_importance.append(df_total_gain, ignore_index=False, sort=False)\n",
    "# df_importance = df_importance.append(df_total_cover, ignore_index=True, sort=False)\n",
    "\n",
    "df_importance.iloc[0] = df_importance.iloc[0]/df_importance.iloc[0].max()\n",
    "df_importance.iloc[1] = df_importance.iloc[1]/df_importance.iloc[1].max()\n",
    "df_importance.iloc[2] = df_importance.iloc[2]/df_importance.iloc[2].max()\n",
    "df_T=df_importance.T\n",
    "df_T.to_csv('importance.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T.plot.bar(figsize=(15,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1 = 'KerasModel.csv'\n",
    "# k1 =pd.read_csv(file1)\n",
    "# file2 = 'XGBModel.csv'\n",
    "# k2 =pd.read_csv(file2)\n",
    "# k2['item_cnt_month'] = (k2['item_cnt_month'] + k1['item_cnt_month'])/2\n",
    "# k2.to_csv('KerasModel_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
