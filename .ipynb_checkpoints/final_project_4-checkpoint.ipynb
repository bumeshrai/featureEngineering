{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "import gc\n",
    "import random as python_random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras seeding to produce reproduciable results\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gc' (built-in)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype in [\"int32\", \"int64\"]]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float16)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int16)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in tqdm(lags):\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Months choosen for lag\n",
    "def lag_features(df, lags, ref_col, index_col, col_to_lag):\n",
    "    '''\n",
    "    ref_col: Reference column relative to which lag is measured\n",
    "    lags: List of lag period\n",
    "    index_col: Columns which will be static\n",
    "    col_to lag: Columns on which lag has to be generated\n",
    "    \n",
    "    The function first makes a dataframe with index column and the columns which are\n",
    "    to be lagged. Then it shifts the reference column forward to the lag period.\n",
    "    Other column in the index remains the same. Then only new columns names are generated\n",
    "    which represents the lag period but the values are same as original.\n",
    "    \n",
    "    Thus the reference column is shifted forward, the column names for lag column are suffixed\n",
    "    with lag period and all other elements remain same. If original value of refernce column is \n",
    "    m, lag required is n. Then new reference value is m+n. The rename column is suffixed '_n'.\n",
    "    The lag column value now shows value of n period back from the refernce perion.\n",
    "    \n",
    "    '''\n",
    "    for month_shift in tqdm(lags):\n",
    "        # Create a temp df\n",
    "        lag_shift = df[index_cols + col_to_lag].copy()\n",
    "\n",
    "        # Shift Month column value\n",
    "        lag_shift[ref_col] = lag_shift[ref_col] + month_shift\n",
    "\n",
    "        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in col_to_lag else x\n",
    "        lag_shift = lag_shift.rename(columns=foo)\n",
    "\n",
    "        # Iteratively add all the lagging months\n",
    "        df = pd.merge(df, lag_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "    \n",
    "    del lag_shift\n",
    "    return df\n",
    "\n",
    "\n",
    "def price_trend(df,col1,col2,col3):\n",
    "    \n",
    "    gb = (df.groupby([col1,col2])[col3].mean()\n",
    "          .reset_index()\n",
    "          .rename(columns = {col3:'avg_price'}))\n",
    "    \n",
    "    gb['shifted_avg_price'] = gb.groupby([col2])['avg_price'].shift(1)\n",
    "    \n",
    "    gb.drop('avg_price', axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.merge(df,gb,on=[col1,col2])\n",
    "    df['price_trend'] = df[col3] - df['shifted_avg_price']\n",
    "    df.drop(['shifted_avg_price'], axis=1, inplace=True)\n",
    "    \n",
    "    del gb\n",
    "    \n",
    "    return df.fillna(0)\n",
    "\n",
    "gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into variable\n",
    "\n",
    "train = pd.read_csv('./final_project_data/sales_train.csv')\n",
    "items = pd.read_csv('./final_project_data/items.csv')\n",
    "category = pd.read_csv('./final_project_data/item_categories.csv')\n",
    "shops = pd.read_csv('./final_project_data/shops.csv')\n",
    "test = pd.read_csv('./final_project_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " before duplicate drop:(214200, 3)  after duplicate drop:(214200, 3)\n"
     ]
    }
   ],
   "source": [
    "# Copy the original dataset to temp \n",
    "sales = train.copy()\n",
    "sales_test = test.copy()\n",
    "\n",
    "sales_index = sales_test['ID']\n",
    "sales_test.drop_duplicates()\n",
    "print(f' before duplicate drop:{test.shape}  after duplicate drop:{sales_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_33 = sales.loc[sales['date_block_num'] == 33]\n",
    "item_sales_last_month = sales_33.groupby(['shop_id','item_id'])['item_cnt_day'].sum().reset_index()\n",
    "item_price_last_month = sales_33.groupby(['shop_id','item_id'])['item_price'].mean().reset_index()\n",
    "# sales_last_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_test['date_block_num'] = 34\n",
    "sales_test = sales_test.merge(item_sales_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\n",
    "sales_test = sales_test.merge(item_price_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\n",
    "sales_test.drop('ID', axis=1, inplace=True)\n",
    "# sales_test.head()\n",
    "sales_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2935849, 6)\n",
      "(3150049, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3150044</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>18454</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150045</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>16188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150046</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>15757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150047</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>19648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150048</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n",
       "3150044  NaN              34       45    18454        99.0           1.0\n",
       "3150045  NaN              34       45    16188         0.0           0.0\n",
       "3150046  NaN              34       45    15757         0.0           0.0\n",
       "3150047  NaN              34       45    19648         0.0           0.0\n",
       "3150048  NaN              34       45      969         0.0           0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sales.shape)\n",
    "sales = sales.append(sales_test, ignore_index=True, sort=False)\n",
    "print(sales.shape)\n",
    "sales.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "#shops\n",
    "shop_city = shops[['shop_id','city_code']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category['split'] = category['item_category_name'].str.split('-')\n",
    "category['type'] = category['split'].map(lambda x: x[0].strip())\n",
    "category['type_code'] = LabelEncoder().fit_transform(category['type'])\n",
    "\n",
    "category['subtype'] = category['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "category['subtype_code'] = LabelEncoder().fit_transform(category['subtype'])\n",
    "category = category[['item_category_id','type_code', 'subtype_code']]\n",
    "\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.merge(sales, item_category_mapping, how='left', on='item_id')\n",
    "\n",
    "sales = pd.merge(sales, category, on=['item_category_id'], how='left')\n",
    "\n",
    "sales = pd.merge(sales, shop_city, on=['shop_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['revenue'] = sales['item_cnt_day'] * sales['item_price']\n",
    "\n",
    "sales['revenue'] = sales['revenue'] / sales['revenue'].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a feature matrix\n",
    "\n",
    "* itertools.product(*iterables):\n",
    "\n",
    "It returns the cartesian product of all the itrable provided as the argument. For example, product(arr1, arr2, arr3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "    # Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby data to get shop-item-month aggregates\n",
    "# Rename the aggregate column to target\n",
    "gb = (sales.groupby(index_cols,as_index=False)['item_cnt_day']\n",
    "          .sum()\n",
    "          .rename(columns = {'item_cnt_day':'target'}))\n",
    "\n",
    "# Join it to the grid\n",
    "# grid is formed by all combination of unique shop,ite,month hence is many more rows,\n",
    "# Joining grid with gb will result in the target column of gb having NaNs\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "\n",
    "all_data = pd.merge(all_data, category, on=['item_category_id'], how='left')\n",
    "\n",
    "all_data = pd.merge(all_data, shop_city, on=['shop_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with shop-month aggregates\n",
    "gb = (sales.groupby(['shop_id', 'date_block_num'],as_index=False)['item_cnt_day']\n",
    "          .sum()\n",
    "          .rename(columns = {'item_cnt_day':'target_shop'}))\n",
    "\n",
    "#gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with item-month aggregates\n",
    "gb = (sales.groupby(['item_id', 'date_block_num'],as_index=False)['item_cnt_day']\n",
    "          .sum().rename(columns = {'item_cnt_day':'target_item'}))\n",
    "\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "#all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Price for each item\n",
    "gb = sales.groupby(index_cols,as_index=False)['item_price'].mean()\n",
    "\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = (sales.groupby(['date_block_num', 'item_category_id'])\n",
    "                 .agg({'item_cnt_day': 'mean'})\n",
    "                 .rename(columns = {'item_cnt_day':'date_cat_avg_item_cnt'})\n",
    "                 .reset_index())\n",
    "\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['date_block_num', 'item_category_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = (sales.groupby(['date_block_num','shop_id','item_category_id'])\n",
    "                 .agg({'item_cnt_day': 'mean'})\n",
    "                 .rename(columns = {'item_cnt_day':'date_shop_cat_avg_item_cnt'})\n",
    "                 .reset_index())\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','shop_id','item_category_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item sold on a type code\n",
    "gb = (sales.groupby(['date_block_num','type_code'])\n",
    "                 .agg({'item_cnt_day': 'mean'})\n",
    "                 .rename(columns = {'item_cnt_day':'date_type_cat_avg_item_cnt'})\n",
    "                 .reset_index())\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','type_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item sold on a subtype code\n",
    "gb = (sales.groupby(['date_block_num','subtype_code'])\n",
    "                 .agg({'item_cnt_day': 'mean'})\n",
    "                 .rename(columns = {'item_cnt_day':'date_subtype_cat_avg_item_cnt'})\n",
    "                 .reset_index())\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','subtype_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = (sales.groupby(['date_block_num', 'city_code'])\n",
    "                 .agg({'item_cnt_day': 'mean'})\n",
    "                 .rename(columns = {'item_cnt_day':'date_city_avg_item_cnt'})\n",
    "                 .reset_index())\n",
    "\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['date_block_num', 'city_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['revenue'] = all_data['target'] * all_data['item_price']\n",
    "\n",
    "all_data['revenue'] = all_data['revenue'] / all_data['revenue'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = (sales.groupby(['date_block_num','revenue'])\n",
    "                 .agg({'item_cnt_day': 'mean'})\n",
    "                 .rename(columns = {'item_cnt_day':'date_revenue_item_cnt'})\n",
    "                 .reset_index())\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','revenue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid\n",
    "del gb, sales, sales_test\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag feature\n",
    "\n",
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:58<00:00, 14.70s/it]\n"
     ]
    }
   ],
   "source": [
    "all_data = lag_features(all_data,[1,2, 3,4], 'date_block_num',index_cols,['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:36<00:00, 18.09s/it]\n"
     ]
    }
   ],
   "source": [
    "all_data = (lag_features(all_data,[1,2], 'date_block_num',index_cols,\n",
    "             ['target_shop','target_item','date_cat_avg_item_cnt','item_price',\n",
    "                  'date_shop_cat_avg_item_cnt', 'date_type_cat_avg_item_cnt', \n",
    "                  'date_subtype_cat_avg_item_cnt']))\n",
    "\n",
    "all_data.drop(['target_shop','target_item','date_cat_avg_item_cnt',\n",
    "                'date_shop_cat_avg_item_cnt', 'date_type_cat_avg_item_cnt', \n",
    "                'date_subtype_cat_avg_item_cnt'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.39s/it]\n"
     ]
    }
   ],
   "source": [
    "all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, \n",
    "                         ['date_revenue_item_cnt']))\n",
    "\n",
    "all_data.drop(['date_revenue_item_cnt','item_price','revenue'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11128050, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, ['date_city_avg_item_cnt']))\n",
    "\n",
    "# all_data.drop('date_city_avg_item_cnt', axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Category for each item\n",
    "\n",
    "\n",
    "all_data = all_data.fillna(0)\n",
    "all_data = downcast_dtypes(all_data)\n",
    "all_data['shop_id'] = all_data['shop_id'].astype(np.int8)\n",
    "all_data['item_id'] = all_data['item_id'].astype(np.int8)\n",
    "all_data['city_code'] = all_data['city_code'].astype(np.int8)\n",
    "all_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11128045</th>\n",
       "      <th>11128046</th>\n",
       "      <th>11128047</th>\n",
       "      <th>11128048</th>\n",
       "      <th>11128049</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shop_id</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>-115.000000</td>\n",
       "      <td>-64.000000</td>\n",
       "      <td>-55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_block_num</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_category_id</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_code</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtype_code</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_code</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_city_avg_item_cnt</th>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.140991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_lag_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_lag_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_lag_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_lag_4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_cat_avg_item_cnt_lag_1</th>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.012695</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.055664</td>\n",
       "      <td>1.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_price_lag_1</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_cat_avg_item_cnt_lag_1</th>\n",
       "      <td>1.024414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.024414</td>\n",
       "      <td>1.015625</td>\n",
       "      <td>1.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_type_cat_avg_item_cnt_lag_1</th>\n",
       "      <td>1.019531</td>\n",
       "      <td>1.652344</td>\n",
       "      <td>1.019531</td>\n",
       "      <td>1.072266</td>\n",
       "      <td>1.072266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_subtype_cat_avg_item_cnt_lag_1</th>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.012695</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>1.055664</td>\n",
       "      <td>1.098633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <td>654.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>654.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_cat_avg_item_cnt_lag_2</th>\n",
       "      <td>1.014648</td>\n",
       "      <td>1.011719</td>\n",
       "      <td>1.014648</td>\n",
       "      <td>1.045898</td>\n",
       "      <td>1.063477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_price_lag_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_cat_avg_item_cnt_lag_2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.017578</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_type_cat_avg_item_cnt_lag_2</th>\n",
       "      <td>1.021484</td>\n",
       "      <td>1.428711</td>\n",
       "      <td>1.021484</td>\n",
       "      <td>1.061523</td>\n",
       "      <td>1.061523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_subtype_cat_avg_item_cnt_lag_2</th>\n",
       "      <td>1.014648</td>\n",
       "      <td>1.011719</td>\n",
       "      <td>1.014648</td>\n",
       "      <td>1.045898</td>\n",
       "      <td>1.094727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_revenue_item_cnt_lag_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       11128045    11128046    11128047  \\\n",
       "shop_id                               45.000000   45.000000   45.000000   \n",
       "item_id                               22.000000   60.000000 -115.000000   \n",
       "date_block_num                        34.000000   34.000000   34.000000   \n",
       "target                                 1.000000    0.000000    0.000000   \n",
       "item_category_id                      55.000000   64.000000   55.000000   \n",
       "type_code                             13.000000   14.000000   13.000000   \n",
       "subtype_code                           2.000000   42.000000    2.000000   \n",
       "city_code                             20.000000   20.000000   20.000000   \n",
       "date_city_avg_item_cnt                 0.140991    0.140991    0.140991   \n",
       "target_lag_1                           1.000000    0.000000    0.000000   \n",
       "target_lag_2                           0.000000    0.000000    0.000000   \n",
       "target_lag_3                           0.000000    0.000000    0.000000   \n",
       "target_lag_4                           0.000000    0.000000    0.000000   \n",
       "target_shop_lag_1                    702.000000  702.000000  702.000000   \n",
       "target_item_lag_1                      2.000000    1.000000    5.000000   \n",
       "date_cat_avg_item_cnt_lag_1            1.022461    1.012695    1.022461   \n",
       "item_price_lag_1                      99.000000    0.000000    0.000000   \n",
       "date_shop_cat_avg_item_cnt_lag_1       1.024414    1.000000    1.024414   \n",
       "date_type_cat_avg_item_cnt_lag_1       1.019531    1.652344    1.019531   \n",
       "date_subtype_cat_avg_item_cnt_lag_1    1.022461    1.012695    1.022461   \n",
       "target_shop_lag_2                    654.000000  654.000000  654.000000   \n",
       "target_item_lag_2                      1.000000    3.000000    3.000000   \n",
       "date_cat_avg_item_cnt_lag_2            1.014648    1.011719    1.014648   \n",
       "item_price_lag_2                       0.000000    0.000000    0.000000   \n",
       "date_shop_cat_avg_item_cnt_lag_2       1.000000    1.000000    1.000000   \n",
       "date_type_cat_avg_item_cnt_lag_2       1.021484    1.428711    1.021484   \n",
       "date_subtype_cat_avg_item_cnt_lag_2    1.014648    1.011719    1.014648   \n",
       "date_revenue_item_cnt_lag_1            0.000000    0.000000    0.000000   \n",
       "\n",
       "                                       11128048    11128049  \n",
       "shop_id                               45.000000   45.000000  \n",
       "item_id                              -64.000000  -55.000000  \n",
       "date_block_num                        34.000000   34.000000  \n",
       "target                                 0.000000    0.000000  \n",
       "item_category_id                      40.000000   37.000000  \n",
       "type_code                             11.000000   11.000000  \n",
       "subtype_code                           4.000000    1.000000  \n",
       "city_code                             20.000000   20.000000  \n",
       "date_city_avg_item_cnt                 0.140991    0.140991  \n",
       "target_lag_1                           0.000000    0.000000  \n",
       "target_lag_2                           0.000000    0.000000  \n",
       "target_lag_3                           0.000000    0.000000  \n",
       "target_lag_4                           0.000000    0.000000  \n",
       "target_shop_lag_1                    702.000000  702.000000  \n",
       "target_item_lag_1                      2.000000    3.000000  \n",
       "date_cat_avg_item_cnt_lag_1            1.055664    1.105469  \n",
       "item_price_lag_1                       0.000000    0.000000  \n",
       "date_shop_cat_avg_item_cnt_lag_1       1.015625    1.022461  \n",
       "date_type_cat_avg_item_cnt_lag_1       1.072266    1.072266  \n",
       "date_subtype_cat_avg_item_cnt_lag_1    1.055664    1.098633  \n",
       "target_shop_lag_2                    654.000000  654.000000  \n",
       "target_item_lag_2                      3.000000    5.000000  \n",
       "date_cat_avg_item_cnt_lag_2            1.045898    1.063477  \n",
       "item_price_lag_2                       0.000000    0.000000  \n",
       "date_shop_cat_avg_item_cnt_lag_2       1.017578    1.000000  \n",
       "date_type_cat_avg_item_cnt_lag_2       1.061523    1.061523  \n",
       "date_subtype_cat_avg_item_cnt_lag_2    1.045898    1.094727  \n",
       "date_revenue_item_cnt_lag_1            0.000000    0.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.tail(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shop_id                                   int8\n",
       "item_id                                   int8\n",
       "date_block_num                            int8\n",
       "target                                 float16\n",
       "item_category_id                         int16\n",
       "type_code                                int16\n",
       "subtype_code                             int16\n",
       "city_code                                 int8\n",
       "date_city_avg_item_cnt                 float16\n",
       "target_lag_1                           float16\n",
       "target_lag_2                           float16\n",
       "target_lag_3                           float16\n",
       "target_lag_4                           float16\n",
       "target_shop_lag_1                      float16\n",
       "target_item_lag_1                      float16\n",
       "date_cat_avg_item_cnt_lag_1            float16\n",
       "item_price_lag_1                       float16\n",
       "date_shop_cat_avg_item_cnt_lag_1       float16\n",
       "date_type_cat_avg_item_cnt_lag_1       float16\n",
       "date_subtype_cat_avg_item_cnt_lag_1    float16\n",
       "target_shop_lag_2                      float16\n",
       "target_item_lag_2                      float16\n",
       "date_cat_avg_item_cnt_lag_2            float16\n",
       "item_price_lag_2                       float16\n",
       "date_shop_cat_avg_item_cnt_lag_2       float16\n",
       "date_type_cat_avg_item_cnt_lag_2       float16\n",
       "date_subtype_cat_avg_item_cnt_lag_2    float16\n",
       "date_revenue_item_cnt_lag_1            float16\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "\n",
    "to_drop_cols = ['target','item_category_id','date_block_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.drop('shop_item', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in ```all_data variable```. Take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/ validation/test split\n",
    "\n",
    "34th month data is the test set. 32nd and 33rd data will be taken as validation split and rest as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "\n",
    "X_train = all_data.loc[(dates <  32)].drop(to_drop_cols, axis=1)\n",
    "X_val = all_data.loc[(dates ==  33) | (dates ==  32)].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == 34].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[(dates <  32), 'target'].values\n",
    "y_val =  all_data.loc[((dates ==  33) | (dates ==  32)), 'target'].values\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu',input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "       layers.Dense(1)\n",
    "      ])\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "    optimizer='rmsprop'\n",
    "    \n",
    "    model.compile(loss=root_mean_squared_error,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model = build_model()\n",
    "\n",
    "# EPOCHS = 10\n",
    "\n",
    "# history = keras_model.fit(\n",
    "#   X_train, y_train,\n",
    "#   epochs=EPOCHS,\n",
    "#     batch_size=100,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     #callbacks=[callbacks],\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# # plt.ylim(bottom=10, top=12)\n",
    "# # plt.xlim(left=1100, right = 1200)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing of test data, fit model\n",
    "# preds_test = keras_model.predict(X_test)\n",
    "\n",
    "# # The prediction is of type numpy.ndarray\n",
    "# preds_list = preds_test.tolist()\n",
    "\n",
    "# # Extract the prediction and put it in a list\n",
    "# prediction = []\n",
    "# for item in preds_list:\n",
    "#     prediction.append(item[0])\n",
    "\n",
    "# prediction = np.clip(prediction, 0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    max_depth=8,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 3)\n",
    "\n",
    "time.time() -start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb_model.predict(X_test).clip(0, 20)\n",
    "\n",
    "prediction = (np.clip(preds, 0, 20)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'ID': sales_index,\n",
    "                       'item_cnt_month': prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv('XbgModel.csv', index=False)\n",
    "# output.to_csv('KerasModel.csv', index=False)\n",
    "output.to_csv('XGBModel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)\n",
    "\n",
    "plot_features(xgb_model, (10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1 = 'KerasModel.csv'\n",
    "# k1 =pd.read_csv(file1)\n",
    "# file2 = 'XGBModel.csv'\n",
    "# k2 =pd.read_csv(file2)\n",
    "# k2['item_cnt_month'] = (k2['item_cnt_month'] + k1['item_cnt_month'])/2\n",
    "# k2.to_csv('KerasModel_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
