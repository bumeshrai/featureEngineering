{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Beware: Works only on Kaggle\n\nHangs on laptop. Needs more RAM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom itertools import product\nimport gc\nimport random as python_random\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keras seeding to produce reproduciable results\nnp.random.seed(123)\npython_random.seed(123)\ntf.random.set_seed(1234)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype in [\"int32\", \"int64\"]]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols]   = df[int_cols].astype(np.int16)\n    \n    return df\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in tqdm(lags):\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n    return df\n\n\n# Months choosen for lag\ndef lag_features(df, lags, ref_col, index_col, col_to_lag):\n    '''\n    ref_col: Reference column relative to which lag is measured\n    lags: List of lag period\n    index_col: Columns which will be static\n    col_to lag: Columns on which lag has to be generated\n    \n    The function first makes a dataframe with index column and the columns which are\n    to be lagged. Then it shifts the reference column forward to the lag period.\n    Other column in the index remains the same. Then only new columns names are generated\n    which represents the lag period but the values are same as original.\n    \n    Thus the reference column is shifted forward, the column names for lag column are suffixed\n    with lag period and all other elements remain same. If original value of refernce column is \n    m, lag required is n. Then new reference value is m+n. The rename column is suffixed '_n'.\n    The lag column value now shows value of n period back from the refernce perion.\n    \n    '''\n    for month_shift in tqdm(lags):\n        # Create a temp df\n        lag_shift = df[index_cols + col_to_lag].copy()\n\n        # Shift Month column value\n        lag_shift[ref_col] = lag_shift[ref_col] + month_shift\n\n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in col_to_lag else x\n        lag_shift = lag_shift.rename(columns=foo)\n\n        # Iteratively add all the lagging months\n        df = pd.merge(df, lag_shift, on=index_cols, how='left').fillna(0)\n\n    \n    del lag_shift\n    return df\n\n\ndef price_trend(df,col1,col2,col3):\n    \n    gb = (df.groupby([col1,col2])[col3].mean()\n          .reset_index()\n          .rename(columns = {col3:'avg_price'}))\n    \n    gb['shifted_avg_price'] = gb.groupby([col2])['avg_price'].shift(1)\n    \n    gb.drop('avg_price', axis=1, inplace=True)\n    \n    df = pd.merge(df,gb,on=[col1,col2])\n    df['price_trend'] = df[col3] - df['shifted_avg_price']\n    df.drop(['shifted_avg_price'], axis=1, inplace=True)\n    \n    del gb\n    \n    return df.fillna(0)\n\ngc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data into variable\n\n## For Kaggle\n\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\ncategory = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\n# For laptop\n\n# train = pd.read_csv('./final_project_data/sales_train.csv')\n# items = pd.read_csv('./final_project_data/items.csv')\n# category = pd.read_csv('./final_project_data/item_categories.csv')\n# shops = pd.read_csv('./final_project_data/shops.csv')\n# test = pd.read_csv('./final_project_data/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the original dataset to temp \nsales = train.copy()\nsales_test = test.copy()\n\nsales_index = sales_test['ID']\nsales_test.drop_duplicates()\nprint(f' before duplicate drop:{test.shape}  after duplicate drop:{sales_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['split'] = sales['date'].str.split('.')\nsales['year'] = sales['split'].map(lambda x: int(x[2]))\nsales['year'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(['date','split'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_33 = sales.loc[sales['date_block_num'] == 33]\nitem_sales_last_month = sales_33.groupby(['shop_id','item_id'])['item_cnt_day'].sum().reset_index()\nitem_price_last_month = sales_33.groupby(['shop_id','item_id'])['item_price'].mean().reset_index()\n# sales_last_month.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test['year'] = 2015\nsales_test['date_block_num'] = 34\nsales_test = sales_test.merge(item_sales_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\nsales_test = sales_test.merge(item_price_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\n\nsales_test.drop('ID', axis=1, inplace=True)\n# sales_test.head()\nsales_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales.shape)\nsales = sales.append(sales_test, ignore_index=True, sort=False)\nprint(sales.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['month'] = sales['date_block_num'] % 12\nsales.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_year = sales[['date_block_num','month','year']]\nmonth_year['date_block_num'].astype(np.int8)\nmonth_year['month'].astype(np.int8)\nmonth_year['year'].astype(np.int16)\nmonth_year.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n#shops\nshop_city = shops[['shop_id','city_code']].drop_duplicates()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category['split'] = category['item_category_name'].str.split('-')\ncategory['type'] = category['split'].map(lambda x: x[0].strip())\ncategory['type_code'] = LabelEncoder().fit_transform(category['type'])\n\ncategory['subtype'] = category['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncategory['subtype_code'] = LabelEncoder().fit_transform(category['subtype'])\ncategory = category[['item_category_id','type_code', 'subtype_code']]\n\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.merge(sales, item_category_mapping, how='left', on='item_id')\n\nsales = pd.merge(sales, category, on=['item_category_id'], how='left')\n\nsales = pd.merge(sales, shop_city, on=['shop_id'], how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['revenue'] = sales['item_cnt_day'] * sales['item_price']\n\nsales['revenue'] = sales['revenue'] / sales['revenue'].max()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get a feature matrix\n\n* itertools.product(*iterables):\n\nIt returns the cartesian product of all the itrable provided as the argument. For example, product(arr1, arr2, arr3).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n    # Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby data to get shop-item-month aggregates\n# Rename the aggregate column to target\ngb = (sales.groupby(index_cols,as_index=False)['item_cnt_day']\n          .sum()\n          .rename(columns = {'item_cnt_day':'target'}))\n\n# Join it to the grid\n# grid is formed by all combination of unique shop,ite,month hence is many more rows,\n# Joining grid with gb will result in the target column of gb having NaNs\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n# all_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n\nall_data = pd.merge(all_data, category, on=['item_category_id'], how='left')\n\nall_data = pd.merge(all_data, shop_city, on=['shop_id'], how='left')\n\ndel item_category_mapping, category,shop_city\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['month'] = all_data['date_block_num'] % 12\nall_data.loc[all_data['date_block_num'] < 13 , 'year'] = 13\nall_data.loc[all_data['date_block_num'] > 24 , 'year'] = 15\nall_data.loc[(all_data['date_block_num'] > 12) & (all_data['date_block_num'] < 25) , 'year'] = 14","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same as above but with shop-month aggregates\ngb = (sales.groupby(['shop_id', 'date_block_num'],as_index=False)['item_cnt_day']\n          .sum()\n          .rename(columns = {'item_cnt_day':'target_shop'}))\n\n#gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same as above but with item-month aggregates\ngb = (sales.groupby(['item_id', 'date_block_num'],as_index=False)['item_cnt_day']\n          .sum().rename(columns = {'item_cnt_day':'target_item'}))\n\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n#all_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Price for each item\ngb = sales.groupby(index_cols,as_index=False)['item_price'].mean()\n\nall_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['date_block_num', 'item_category_id'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'date_cat_avg_item_cnt'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num', 'item_category_id'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['date_block_num','shop_id','item_category_id'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'date_shop_cat_avg_item_cnt'})\n                 .reset_index())\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','shop_id','item_category_id'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Item sold on a type code\ngb = (sales.groupby(['date_block_num','type_code'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'date_type_cat_avg_item_cnt'})\n                 .reset_index())\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','type_code'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Item sold on a subtype code\ngb = (sales.groupby(['date_block_num','subtype_code'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'date_subtype_cat_avg_item_cnt'})\n                 .reset_index())\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','subtype_code'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['date_block_num', 'city_code'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'date_city_avg_item_cnt'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num', 'city_code'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['revenue'] = all_data['target'] * all_data['item_price']\n\nall_data['revenue'] = all_data['revenue'] / all_data['revenue'].max()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['date_block_num','revenue'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'date_revenue_item_cnt'})\n                 .reset_index())\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','revenue'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb = (sales.groupby(['date_block_num', 'item_id'])\n#                  .agg({'item_cnt_day': 'sum'})\n#                  .rename(columns = {'item_cnt_day':'date_item_sale'})\n#                  .reset_index())\n\n# all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','item_id'])\n\n# gb = (gb.groupby(['item_id'])\n#                  .agg({'date_item_sale': 'mean'})\n#                  .rename(columns = {'date_item_sale':'item_avg_sale'})\n#                  .reset_index())\n\n# all_data = pd.merge(all_data, gb, how='left', on=['item_id'])\n# all_data['delta_item'] = (all_data['date_item_sale'] - all_data['item_avg_sale']) / all_data['item_avg_sale']\n\n# all_data.drop(['date_item_sale','item_avg_sale'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['date_block_num', 'shop_id'])\n                 .agg({'item_cnt_day': 'sum'})\n                 .rename(columns = {'item_cnt_day':'date_shop_sale'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','shop_id'])\n\ngb = (gb.groupby(['shop_id'])\n                 .agg({'date_shop_sale': 'mean'})\n                 .rename(columns = {'date_shop_sale':'shop_avg_sale'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id'])\nall_data['delta_sale'] = (all_data['date_shop_sale'] - all_data['shop_avg_sale']) / all_data['shop_avg_sale']\n\nall_data.drop(['date_shop_sale','shop_avg_sale'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['date_block_num', 'shop_id'])\n                 .agg({'revenue': 'sum'})\n                 .rename(columns = {'revenue':'date_shop_revenue'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','shop_id'])\n\ngb = (gb.groupby(['shop_id'])\n                 .agg({'date_shop_revenue': 'mean'})\n                 .rename(columns = {'date_shop_revenue':'shop_avg_revenue'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id'])\nall_data['delta_revenue'] = ((all_data['date_shop_revenue'] - \n                                  all_data['shop_avg_revenue']) / all_data['shop_avg_revenue'])\n\n# all_data.drop(['date_shop_revenue','shop_avg_revenue'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = (sales.groupby(['month','shop_id'])\n                 .agg({'item_cnt_day': 'mean'})\n                 .rename(columns = {'item_cnt_day':'month_cnt_shop'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['month','shop_id']).fillna(0)\n\ngb = (gb.groupby(['shop_id'])\n                 .agg({'month_cnt_shop': 'mean'})\n                 .rename(columns = {'month_cnt_shop':'month_shop_avg_cnt'})\n                 .reset_index())\n\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id'])\nall_data['delta_month'] = ((all_data['month_cnt_shop'] - \n                                  all_data['month_shop_avg_cnt']) / all_data['month_shop_avg_cnt'])\n\n# all_data.drop(['month_cnt_shop','month_shop_avg_cnt'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb = (sales.groupby(['month','shop_id','type_code'])\n#                  .agg({'revenue': 'mean'})\n#                  .rename(columns = {'revenue':'month_cnt_shop_type'})\n#                  .reset_index())\n\n# all_data = pd.merge(all_data, gb, how='left', on=['month','shop_id','type_code']).fillna(0)\n\n# gb = (gb.groupby(['shop_id','type_code'])\n#                  .agg({'month_cnt_shop_type': 'mean'})\n#                  .rename(columns = {'month_cnt_shop_type':'month_shop_type_avg_cnt'})\n#                  .reset_index())\n\n# all_data = pd.merge(all_data, gb, how='left', on=['shop_id','type_code'])\n# all_data['delta_month'] = ((all_data['month_cnt_shop_type'] - \n#                                   all_data['month_shop_type_avg_cnt']) / all_data['month_shop_type_avg_cnt'])\n\n# all_data.drop(['month_cnt_shop_type','month_shop_type_avg_cnt'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb = (sales.groupby(['date_block_num', 'shop_id', 'item_id'])\n#                  .agg({'item_cnt_day': 'sum'})\n#                  .rename(columns = {'item_cnt_day':'date_shop_item_sale'})\n#                  .reset_index())\n\n# all_data = pd.merge(all_data, gb, how='left', on=['date_block_num','shop_id', 'item_id'])\n\n# gb = (gb.groupby(['shop_id','item_id'])\n#                  .agg({'date_shop_item_sale': 'mean'})\n#                  .rename(columns = {'date_shop_item_sale':'shop_item_avg_sale'})\n#                  .reset_index())\n\n# all_data = pd.merge(all_data, gb, how='left', on=['shop_id','item_id'])\n# all_data['delta_sale_item'] = ((all_data['date_shop_item_sale'] - \n#                                    all_data['shop_item_avg_sale']) / all_data['date_shop_item_sale'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downcast dtypes from 64 to 32 bit to save memory\ndel grid\ndel gb, sales, sales_test\ngc.collect();\n\nall_data = downcast_dtypes(all_data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lag feature\n\nAfter creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = lag_features(all_data,[1,2, 3, 4], 'date_block_num',index_cols,['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = (lag_features(all_data,[1,2], 'date_block_num',index_cols,\n             ['target_shop','target_item','date_cat_avg_item_cnt','item_price',\n                  'date_shop_cat_avg_item_cnt', 'date_type_cat_avg_item_cnt', \n                  'date_subtype_cat_avg_item_cnt']))\n\nall_data.drop(['target_shop','target_item','date_cat_avg_item_cnt',\n                'date_shop_cat_avg_item_cnt', 'date_type_cat_avg_item_cnt', \n                'date_subtype_cat_avg_item_cnt'], axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### try out","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, \n                        ['date_revenue_item_cnt']))\n\nall_data.drop(['date_revenue_item_cnt','revenue','item_price','month', 'year'], axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, ['date_city_avg_item_cnt']))\n\n# all_data.drop('revenue', axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Category for each item\n\n\nall_data = all_data.fillna(0)\nall_data = downcast_dtypes(all_data)\nall_data['shop_id'] = all_data['shop_id'].astype(np.int8)\nall_data['item_id'] = all_data['item_id'].astype(np.int8)\nall_data['city_code'] = all_data['city_code'].astype(np.int8)\nall_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.tail(5).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n\nto_drop_cols = ['target','item_category_id','date_block_num']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data.drop('shop_item', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To this end, we've created a feature matrix. It is stored in ```all_data variable```. Take a look:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### train/ validation/test split\n\n34th month data is the test set. 32nd and 33rd data will be taken as validation split and rest as training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \ndates = all_data['date_block_num']\n\n\nX_train = all_data.loc[(dates <  32)].drop(to_drop_cols, axis=1)\nX_val = all_data.loc[(dates ==  33) | (dates ==  32)].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == 34].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[(dates <  32), 'target'].values\ny_val =  all_data.loc[((dates ==  33) | (dates ==  32)), 'target'].values\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    model = keras.Sequential([\n        layers.Dense(128, activation='relu',input_shape=[X_train.shape[1]]),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(64, activation='relu'),\n       layers.Dense(1)\n      ])\n\n    #optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n    optimizer='rmsprop'\n    \n    model.compile(loss=root_mean_squared_error,\n                optimizer=optimizer,\n                metrics=['mse',])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras_model = build_model()\n\n# EPOCHS = 10\n\n# history = keras_model.fit(\n#   X_train, y_train,\n#   epochs=EPOCHS,\n#     batch_size=100,\n#     validation_data=(X_val, y_val),\n#     #callbacks=[callbacks],\n#   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Model accuracy')\n# plt.ylabel('loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# # plt.ylim(bottom=10, top=12)\n# # plt.xlim(left=1100, right = 1200)\n# plt.grid()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Preprocessing of test data, fit model\n# preds_test = keras_model.predict(X_test)\n\n# # The prediction is of type numpy.ndarray\n# preds_list = preds_test.tolist()\n\n# # Extract the prediction and put it in a list\n# prediction = []\n# for item in preds_list:\n#     prediction.append(item[0])\n\n# prediction = np.clip(prediction, 0, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nxgb_model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 3)\n\ntime.time() -start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = xgb_model.predict(X_test).clip(0, 20)\n\nprediction = (np.clip(preds, 0, 20)).tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'ID': sales_index,\n                       'item_cnt_month': prediction})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output.to_csv('XbgModel.csv', index=False)\n# output.to_csv('KerasModel.csv', index=False)\noutput.to_csv('XGBModel.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xgb_model, (10,14))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_gain = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='gain'), index=['gain'])\ndf_weight = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='weight'), index=['weight'])\ndf_cover = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='cover'), index=['cover'])\n# df_total_gain = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='total_gain'), index=['total_gain'])\n# df_total_cover = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='total_cover'), index=['total_cover'])\n\ndf_importance = df_gain.append(df_weight, ignore_index=False, sort=False)\ndf_importance = df_importance.append(df_cover, ignore_index=False, sort=False)\n# df_importance = df_importance.append(df_total_gain, ignore_index=False, sort=False)\n# df_importance = df_importance.append(df_total_cover, ignore_index=True, sort=False)\n\ndf_importance.iloc[0] = df_importance.iloc[0]/df_importance.iloc[0].max()\ndf_importance.iloc[1] = df_importance.iloc[1]/df_importance.iloc[1].max()\ndf_importance.iloc[2] = df_importance.iloc[2]/df_importance.iloc[2].max()\ndf_T=df_importance.T\ndf_T.to_csv('importance.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_T.plot.bar(figsize=(15,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_T.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# file1 = 'KerasModel.csv'\n# k1 =pd.read_csv(file1)\n# file2 = 'XGBModel.csv'\n# k2 =pd.read_csv(file2)\n# k2['item_cnt_month'] = (k2['item_cnt_month'] + k1['item_cnt_month'])/2\n# k2.to_csv('KerasModel_mean.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}