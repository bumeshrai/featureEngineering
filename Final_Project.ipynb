{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Beware: Works only on Kaggle\n\nHangs on laptop. Needs more RAM."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import Pool, CatBoostRegressor\n\n\nimport lightgbm as lgb\n\nfrom itertools import product\nimport gc\nimport random as python_random\nimport time\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keras seeding to produce reproduciable results\nnp.random.seed(123)\npython_random.seed(123)\ntf.random.set_seed(1234)\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype in [\"int32\", \"int64\"]]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols]   = df[int_cols].astype(np.int16)\n    \n    return df\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in tqdm(lags):\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n    return df\n\n\n# Months choosen for lag\ndef lag_features(df, lags, ref_col, index_col, col_to_lag):\n    '''\n    ref_col: Reference column relative to which lag is measured\n    lags: List of lag period\n    index_col: List of columns which will be static\n    col_to lag: List of columns on which lag has to be generated\n    \n    The function first makes a dataframe with index column and the columns which are\n    to be lagged. Then it shifts the reference column forward to the lag period.\n    Other column in the index remains the same. Then only new columns names are generated\n    which represents the lag period but the values are same as original.\n    \n    Thus the reference column is shifted forward, the column names for lag column are suffixed\n    with lag period and all other elements remain same. If original value of refernce column is \n    m, lag required is n. Then new reference value is m+n. The rename column is suffixed '_n'.\n    The lag column value now shows value of n period back from the refernce perion.\n    \n    '''\n    #print(col_to_lag)\n    for month_shift in tqdm(lags):\n        # Create a temp df\n        lag_shift = df[index_cols + col_to_lag].copy()\n\n        # Shift Month column value\n        lag_shift[ref_col] = lag_shift[ref_col] + month_shift\n        \n#         print(lag_shift.head(2))\n\n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in col_to_lag else x\n        lag_shift = lag_shift.rename(columns=foo)\n\n        # Iteratively add all the lagging months\n        df = pd.merge(df, lag_shift, on=index_cols, how='left').fillna(0)\n\n    \n    del lag_shift\n    return df\n\n\ndef price_trend(df,col1,col2,col3):\n    '''\n    Returns a column of trend. True if it has gone up, False otherewise.\n    \n    col1 - Time column\n    col2 - Item which is monitored\n    col3 - units of items \n    '''\n    \n    trend_col = col1[:4] + '_' + col2[:4] + '_' + col3[:4] + '_trend'\n    gb = (df.groupby([col1,col2])[col3].sum()\n          .reset_index()\n          .rename(columns = {col3:'avg_price'}))\n    \n    gb['shifted_avg_price'] = gb.groupby([col2])['avg_price'].shift(1)\n    \n    gb.drop('avg_price', axis=1, inplace=True)\n    \n    df = pd.merge(df,gb,on=[col1,col2])\n    df['price_trend'] = df[col3] - df['shifted_avg_price']\n    df[trend_col] = df['price_trend'].map(lambda x: x>0)\n    df.drop(['shifted_avg_price', 'price_trend'], axis=1, inplace=True)\n    \n    del gb\n    \n    return (trend_col, df.fillna(0))\n\n\n# Measures the delta changes in a column with respect to other columns.\ndef delta_change(df1, df2, gb_cols, pivot_col, agg_col, agg_func='sum', drop_col=True):\n    '''\n    gb_cols are the index columns w.r.t which the changes will be calculated\n    pivot_col is the column subset of the index column relative to which the\n    change is computed\n    agg_col is the column which is monitored for change\n    agg_func is the function (sum, mean...) which will be used for aggregation\n    drop col is the boolean to know whether the aggregator column and the averaging\n    column to be retained or not.\n    \n    The function first aggregates a column on the groupby statement of index columns of the\n    training dataframe. Then the aggregrated column average is computed for the pivot column. \n    This mean is then subtracted from the aggregated column to get the delta column for the \n    new dataframe being built.\n    '''\n    \n    aggregated_col = '_'.join(gb_cols) + '_agg'\n    averaged_col = '_'.join(gb_cols) + '_avg_agg'\n    delta_col = 'delta_' + '_'.join(gb_cols) + '_' + agg_col\n    \n    gb = (df1.groupby(gb_cols)\n                 .agg({agg_col: agg_func})\n                 .rename(columns = {agg_col:aggregated_col})\n                 .reset_index())\n\n    df2 = pd.merge(df2, gb, how='left', on=gb_cols)\n\n    gb = (gb.groupby(pivot_col)\n                 .agg({aggregated_col: 'mean'})\n                 .rename(columns = {aggregated_col:averaged_col})\n                 .reset_index())\n\n    df2 = pd.merge(df2, gb, how='left', on=[pivot_col])\n    df2[delta_col] = ((df2[aggregated_col] - \n                                  df2[averaged_col]) / df2[averaged_col])\n    \n    if drop_col:\n        df2.drop([aggregated_col,averaged_col], axis=1,inplace=True)\n    \n    del gb\n    \n    return df2\n\n\ndef me_gb(df1, df2, gb_cols, agg_col, agg_func='sum', rename_specific = False, new_name = ''):\n    '''\n    gb_cols are the index columns w.r.t which the changes will be calculated\n    \n    agg_col is the column which is monitored for change\n    agg_func is the function (sum, mean...) which will be used for aggregation\n    rename_specific is the boolean to know whether the aggregator column  has to be\n    renamed\n    new_name is the new name that is to be given if changed\n    \n    The function first aggregates a column on the groupby statement of index columns of the\n    training dataframe. Then it is merged to the new dataframe that is being built.\n    \n    '''\n    # Get first 4 characters\n    list_derived = []\n    for i in gb_cols:\n        list_derived.append(i[:4])\n    \n    if rename_specific:\n        aggregated_col = new_name\n    else:\n        aggregated_col = '_'.join(list_derived) + '_' + agg_col[-3:]\n        \n    \n    \n    gb = (df1.groupby(gb_cols)\n                 .agg({agg_col: agg_func})\n                 .rename(columns = {agg_col:aggregated_col})\n                 .reset_index())\n\n    df2 = pd.merge(df2, gb, how='left', on=gb_cols)\n\n    del gb\n    \n    return (aggregated_col, df2.fillna(0))\n\ndef f2s(df):\n    float_cols = [c for c in df if df[c].dtype in ['float16', 'float32', 'float64']]\n    for cols in float_cols:\n        df[cols] = df[cols].map(lambda x: (str(x))[:4])\n    \n    return df\n\ngc","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<module 'gc' (built-in)>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data into variable\n\n## For Kaggle\n\ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\ncategory = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\n# For laptop\n\n# train = pd.read_csv('./final_project_data/sales_train.csv')\n# items = pd.read_csv('./final_project_data/items.csv')\n# category = pd.read_csv('./final_project_data/item_categories.csv')\n# shops = pd.read_csv('./final_project_data/shops.csv')\n# test = pd.read_csv('./final_project_data/test.csv')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the original dataset to temp \nsales = train.copy()\nsales_test = test.copy()\n\nsales_index = sales_test['ID']\nsales_test.drop_duplicates()\nprint(f' before duplicate drop:{test.shape}  after duplicate drop:{sales_test.shape}')","execution_count":5,"outputs":[{"output_type":"stream","text":" before duplicate drop:(214200, 3)  after duplicate drop:(214200, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['split'] = sales['date'].str.split('.')\nsales['year'] = sales['split'].map(lambda x: int(x[2][-2:]))\nsales['year'] = sales['year'].astype(np.int8)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(['date','split'], axis=1, inplace=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_33 = sales.loc[sales['date_block_num'] == 33]\nitem_sales_last_month = sales_33.groupby(['shop_id','item_id'])['item_cnt_day'].sum().reset_index()\nitem_price_last_month = sales_33.groupby(['shop_id','item_id'])['item_price'].mean().reset_index()\n# sales_last_month.head()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test['year'] = 2015\nsales_test['date_block_num'] = 34\nsales_test = sales_test.merge(item_sales_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\nsales_test = sales_test.merge(item_price_last_month, on=['shop_id', 'item_id'], how='left').fillna(0)\n\nsales_test.drop('ID', axis=1, inplace=True)\n# sales_test.head()\nsales_test.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(214200, 6)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(sales.shape)\nsales = sales.append(sales_test, ignore_index=True, sort=False)\n# print(sales.shape)\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['month'] = sales['date_block_num'] % 12\nsales.tail()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"         date_block_num  shop_id  item_id  item_price  item_cnt_day  year  \\\n3150044              34       45    18454        99.0           1.0  2015   \n3150045              34       45    16188         0.0           0.0  2015   \n3150046              34       45    15757         0.0           0.0  2015   \n3150047              34       45    19648         0.0           0.0  2015   \n3150048              34       45      969         0.0           0.0  2015   \n\n         month  \n3150044     10  \n3150045     10  \n3150046     10  \n3150047     10  \n3150048     10  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_block_num</th>\n      <th>shop_id</th>\n      <th>item_id</th>\n      <th>item_price</th>\n      <th>item_cnt_day</th>\n      <th>year</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3150044</th>\n      <td>34</td>\n      <td>45</td>\n      <td>18454</td>\n      <td>99.0</td>\n      <td>1.0</td>\n      <td>2015</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3150045</th>\n      <td>34</td>\n      <td>45</td>\n      <td>16188</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3150046</th>\n      <td>34</td>\n      <td>45</td>\n      <td>15757</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3150047</th>\n      <td>34</td>\n      <td>45</td>\n      <td>19648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3150048</th>\n      <td>34</td>\n      <td>45</td>\n      <td>969</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_year = sales[['date_block_num','month','year']].drop_duplicates()\nmonth_year['date_block_num'].astype(np.int8)\nmonth_year['month'].astype(np.int8)\nmonth_year['year'].astype(np.int16)\nmonth_year.sample(5)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"         date_block_num  month  year\n631921                6      6    13\n2719169              30      6    15\n531518                5      5    13\n1267562              12      0    14\n2323423              24      0    15","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_block_num</th>\n      <th>month</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>631921</th>\n      <td>6</td>\n      <td>6</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2719169</th>\n      <td>30</td>\n      <td>6</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>531518</th>\n      <td>5</td>\n      <td>5</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1267562</th>\n      <td>12</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2323423</th>\n      <td>24</td>\n      <td>0</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n#shops\nshop_city = shops[['shop_id','city_code']].drop_duplicates()\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category['split'] = category['item_category_name'].str.split('-')\ncategory['type'] = category['split'].map(lambda x: x[0].strip())\ncategory['type_code'] = LabelEncoder().fit_transform(category['type'])\n\ncategory['subtype'] = category['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncategory['subtype_code'] = LabelEncoder().fit_transform(category['subtype'])\ncategory = category[['item_category_id','type_code', 'subtype_code']]\n\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.merge(sales, item_category_mapping, how='left', on='item_id')\n\nsales = pd.merge(sales, category, on=['item_category_id'], how='left')\n\nsales = pd.merge(sales, shop_city, on=['shop_id'], how='left')\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['revenue'] = sales['item_cnt_day'] * sales['item_price']\n\nsales['revenue'] = sales['revenue'] / sales['revenue'].max()\n","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get a feature matrix\n\n* itertools.product(*iterables):\n\nIt returns the cartesian product of all the itrable provided as the argument. For example, product(arr1, arr2, arr3)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create \"grid\" with columns\nindex_cols = ['date_block_num', 'shop_id', 'item_id']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[[block_num], cur_shops, cur_items ])),dtype='int16'))\n\n    # Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int16)\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join it to the grid\n# grid is formed by all combination of unique shop,ite,month hence is many more rows,\n\nall_data = pd.merge(grid, item_category_mapping, how='left', on='item_id')\n\nall_data = pd.merge(all_data, category, on=['item_category_id'], how='left')\n\nall_data = pd.merge(all_data, shop_city, on=['shop_id'], how='left')\n\nall_data = pd.merge(all_data, month_year, on=['date_block_num'], how='left')\n\n\ndel item_category_mapping, category, shop_city, month_year\n\ngc.collect()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"35"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Mean Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id', 'item_id'], \n                            'item_cnt_day', agg_func='sum', rename_specific = True, new_name = 'target'))\n\nlist_1_lag = [ret_col]","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id'], \n                            'item_cnt_day', agg_func='sum', rename_specific = True, new_name = 'target_shop'))\n\nlist_2_lag = [ret_col]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'item_id'], \n                            'item_cnt_day', agg_func='sum', rename_specific = True, new_name = 'target_item'))\n\nlist_2_lag.append(ret_col)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'item_category_id'], \n                            'item_cnt_day', agg_func='mean'))\n# \nlist_2_lag.append(ret_col)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id', 'item_id'], \n                            'item_price', agg_func='mean', rename_specific = True, new_name = 'item_price'))\n\nlist_2_lag.append(ret_col)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['revenue'] = all_data['target'] * all_data['item_price']\n\nall_data['revenue'] = all_data['revenue'] / all_data['revenue'].max()\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_day', \n                            agg_func='mean'))\n\nlist_2_lag.append(ret_col)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'type_code'], 'item_cnt_day', \n                           agg_func='mean'))\n\nlist_2_lag.append(ret_col)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'subtype_code'], 'item_cnt_day', \n                           agg_func='mean', ))\n\nlist_2_lag.append(ret_col)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'city_code'], 'item_cnt_day', \n                           agg_func='mean'))\n\nlist_2_lag.append(ret_col)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'shop_id'], 'revenue', \n                            agg_func='sum'))\n\nlist_2_lag.append(ret_col)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'item_category_id'], 'revenue', \n                            agg_func='sum'))\n\nlist_2_lag.append(ret_col)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'subtype_code'], 'revenue', \n                            agg_func='sum'))\n\nlist_2_lag.append(ret_col)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'type_code'], 'revenue', \n                            agg_func='sum'))\n\nlist_2_lag.append(ret_col)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ret_col, all_data = (me_gb(sales, all_data, ['date_block_num', 'revenue'], 'item_cnt_day', \n#                            agg_func='sum'))\n\n# list_2_lag.append(ret_col)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = delta_change(sales, all_data, ['date_block_num', 'shop_id'], 'shop_id', 'item_cnt_day')\nall_data = delta_change(sales, all_data, ['date_block_num', 'shop_id'], 'shop_id', 'revenue', drop_col=False)\nall_data = delta_change(sales, all_data, ['month', 'shop_id'], 'shop_id', 'item_cnt_day', 'mean', False)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_3_lag = []\n\nret_col, all_data = price_trend(all_data,'date_block_num','shop_id','target')\nlist_3_lag.append(ret_col)\n\n# ret_col, all_data = price_trend(all_data,'date_block_num','item_category_id','target')\n# list_3_lag.append(ret_col)\n\n# ret_col, all_data = price_trend(all_data,'date_block_num','type_code','target')\n# list_3_lag.append(ret_col)\n\n# ret_col, all_data = price_trend(all_data,'date_block_num','subtype_code','target')\n# list_3_lag.append(ret_col)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data = price_trend(all_data,'month','shop_id','revenue')","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downcast dtypes from 64 to 32 bit to save memory\ndel grid\ndel sales, sales_test\ngc.collect();\n\nall_data = downcast_dtypes(all_data)\n","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lag feature\n\nAfter creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = lag_features(all_data,[1,2, 3, 4], 'date_block_num',index_cols,list_1_lag)","execution_count":38,"outputs":[{"output_type":"stream","text":"100%|██████████| 4/4 [00:46<00:00, 11.51s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data = lag_features(all_data,[1,2], 'date_block_num',index_cols,['target_shop'])","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = (lag_features(all_data,[1,2], 'date_block_num',index_cols,list_2_lag + list_3_lag  ))","execution_count":40,"outputs":[{"output_type":"stream","text":"100%|██████████| 2/2 [00:48<00:00, 24.35s/it]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"#### try out"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in list_3_lag:\n    col_name = col+'_lag_1'\n    all_data[col_name] = all_data[col_name].map(lambda x: x > 0)\n    col_name = col+'_lag_2'\n    all_data[col_name] = all_data[col_name].map(lambda x: x > 0)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nall_data.drop(list_2_lag  + list_3_lag  + ['revenue','month', 'year', 'city_code'], axis=1,inplace=True)\n\nall_data.shape","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"(11128050, 45)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data = (lag_features(all_data,[1], 'date_block_num',index_cols, ['date_city_item_cnt']))\n\n# all_data.drop('revenue', axis=1,inplace=True)\n","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data['price_trend_lag_1'] = all_data['price_trend_lag_1'].map(lambda x: x > 0)\n# all_data['price_trend_lag_2'] = all_data['price_trend_lag_2'].map(lambda x: x > 0)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Category for each item\n\n\nall_data = all_data.fillna(0)\nall_data = downcast_dtypes(all_data)\nall_data['shop_id'] = all_data['shop_id'].astype(np.int8)\nall_data['item_id'] = all_data['item_id'].astype(np.int8)\n# all_data['city_code'] = all_data['city_code'].astype(np.int8)\nall_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)\n\ngc.collect();","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.sample(2).T","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"                                           10980671  4923796 \ndate_block_num                                   34        13\nshop_id                                          24        12\nitem_id                                         -41       121\nitem_category_id                                 30        32\ntype_code                                         8        10\nsubtype_code                                     55        34\ntarget                                            0         0\ndelta_date_block_num_shop_id_item_cnt_day -0.348877 -0.121033\ndate_block_num_shop_id_agg                 0.956543    3.2793\ndate_block_num_shop_id_avg_agg              1.10156   1.79492\ndelta_date_block_num_shop_id_revenue      -0.131714  0.827637\nmonth_shop_id_agg                           0.68457   1.62695\nmonth_shop_id_avg_agg                       1.15234   2.01953\ndelta_month_shop_id_item_cnt_day           -0.40625  -0.19397\ndate_shop_targ_trend                          False     False\ntarget_lag_1                                      0         0\ntarget_lag_2                                      0         0\ntarget_lag_3                                      0         0\ntarget_lag_4                                      1         0\ntarget_shop_lag_1                              1227      1103\ntarget_item_lag_1                                 9        28\ndate_item_day_lag_1                         1.15527   1.28809\nitem_price_lag_1                                  0         0\ndate_shop_item_day_lag_1                    1.28613         0\ndate_type_day_lag_1                          1.3457   1.28809\ndate_subt_day_lag_1                         1.15527   1.28809\ndate_city_day_lag_1                         1.34961   1.20508\ndate_shop_nue_lag_1                        0.958984  0.714844\ndate_item_nue_lag_1                         1.48828  0.192383\ndate_subt_nue_lag_1                         1.48828  0.192383\ndate_type_nue_lag_1                               4  0.192383\ndate_shop_targ_trend_lag_1                    False     False\ntarget_shop_lag_2                              1492      2620\ntarget_item_lag_2                                20        55\ndate_item_day_lag_2                         1.23438   1.33008\nitem_price_lag_2                                  0         0\ndate_shop_item_day_lag_2                    1.29492         0\ndate_type_day_lag_2                          1.3125   1.33008\ndate_subt_day_lag_2                         1.23438   1.33008\ndate_city_day_lag_2                         1.39062   1.64941\ndate_shop_nue_lag_2                         1.18555   5.34375\ndate_item_nue_lag_2                          1.8252  0.315918\ndate_subt_nue_lag_2                          1.8252  0.315918\ndate_type_nue_lag_2                         4.24219  0.315918\ndate_shop_targ_trend_lag_2                    False     False","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>10980671</th>\n      <th>4923796</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>date_block_num</th>\n      <td>34</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>shop_id</th>\n      <td>24</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>item_id</th>\n      <td>-41</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>item_category_id</th>\n      <td>30</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>type_code</th>\n      <td>8</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>subtype_code</th>\n      <td>55</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>delta_date_block_num_shop_id_item_cnt_day</th>\n      <td>-0.348877</td>\n      <td>-0.121033</td>\n    </tr>\n    <tr>\n      <th>date_block_num_shop_id_agg</th>\n      <td>0.956543</td>\n      <td>3.2793</td>\n    </tr>\n    <tr>\n      <th>date_block_num_shop_id_avg_agg</th>\n      <td>1.10156</td>\n      <td>1.79492</td>\n    </tr>\n    <tr>\n      <th>delta_date_block_num_shop_id_revenue</th>\n      <td>-0.131714</td>\n      <td>0.827637</td>\n    </tr>\n    <tr>\n      <th>month_shop_id_agg</th>\n      <td>0.68457</td>\n      <td>1.62695</td>\n    </tr>\n    <tr>\n      <th>month_shop_id_avg_agg</th>\n      <td>1.15234</td>\n      <td>2.01953</td>\n    </tr>\n    <tr>\n      <th>delta_month_shop_id_item_cnt_day</th>\n      <td>-0.40625</td>\n      <td>-0.19397</td>\n    </tr>\n    <tr>\n      <th>date_shop_targ_trend</th>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>target_lag_1</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>target_lag_2</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>target_lag_3</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>target_lag_4</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>target_shop_lag_1</th>\n      <td>1227</td>\n      <td>1103</td>\n    </tr>\n    <tr>\n      <th>target_item_lag_1</th>\n      <td>9</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>date_item_day_lag_1</th>\n      <td>1.15527</td>\n      <td>1.28809</td>\n    </tr>\n    <tr>\n      <th>item_price_lag_1</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>date_shop_item_day_lag_1</th>\n      <td>1.28613</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>date_type_day_lag_1</th>\n      <td>1.3457</td>\n      <td>1.28809</td>\n    </tr>\n    <tr>\n      <th>date_subt_day_lag_1</th>\n      <td>1.15527</td>\n      <td>1.28809</td>\n    </tr>\n    <tr>\n      <th>date_city_day_lag_1</th>\n      <td>1.34961</td>\n      <td>1.20508</td>\n    </tr>\n    <tr>\n      <th>date_shop_nue_lag_1</th>\n      <td>0.958984</td>\n      <td>0.714844</td>\n    </tr>\n    <tr>\n      <th>date_item_nue_lag_1</th>\n      <td>1.48828</td>\n      <td>0.192383</td>\n    </tr>\n    <tr>\n      <th>date_subt_nue_lag_1</th>\n      <td>1.48828</td>\n      <td>0.192383</td>\n    </tr>\n    <tr>\n      <th>date_type_nue_lag_1</th>\n      <td>4</td>\n      <td>0.192383</td>\n    </tr>\n    <tr>\n      <th>date_shop_targ_trend_lag_1</th>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>target_shop_lag_2</th>\n      <td>1492</td>\n      <td>2620</td>\n    </tr>\n    <tr>\n      <th>target_item_lag_2</th>\n      <td>20</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>date_item_day_lag_2</th>\n      <td>1.23438</td>\n      <td>1.33008</td>\n    </tr>\n    <tr>\n      <th>item_price_lag_2</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>date_shop_item_day_lag_2</th>\n      <td>1.29492</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>date_type_day_lag_2</th>\n      <td>1.3125</td>\n      <td>1.33008</td>\n    </tr>\n    <tr>\n      <th>date_subt_day_lag_2</th>\n      <td>1.23438</td>\n      <td>1.33008</td>\n    </tr>\n    <tr>\n      <th>date_city_day_lag_2</th>\n      <td>1.39062</td>\n      <td>1.64941</td>\n    </tr>\n    <tr>\n      <th>date_shop_nue_lag_2</th>\n      <td>1.18555</td>\n      <td>5.34375</td>\n    </tr>\n    <tr>\n      <th>date_item_nue_lag_2</th>\n      <td>1.8252</td>\n      <td>0.315918</td>\n    </tr>\n    <tr>\n      <th>date_subt_nue_lag_2</th>\n      <td>1.8252</td>\n      <td>0.315918</td>\n    </tr>\n    <tr>\n      <th>date_type_nue_lag_2</th>\n      <td>4.24219</td>\n      <td>0.315918</td>\n    </tr>\n    <tr>\n      <th>date_shop_targ_trend_lag_2</th>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.to_pickle(\"all_data.pkl\")\n\nall_data = pd.read_pickle(\"all_data.pkl\")","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n\nto_drop_cols = ['target','item_category_id','date_block_num']","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data.drop('shop_item', axis=1, inplace=True)","execution_count":49,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train/ validation/test split\n\n34th month data is the test set. 32nd and 33rd data will be taken as validation split and rest as training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \ndates = all_data['date_block_num']\n\n\nX_train = all_data.loc[(dates <  32)].drop(to_drop_cols, axis=1)\nX_val = all_data.loc[(dates ==  33) | (dates ==  32)].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == 34].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[(dates <  32), 'target'].values\ny_val =  all_data.loc[((dates ==  33) | (dates ==  32)), 'target'].values\n\ngc.collect();","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"(214200, 42)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Keras Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n","execution_count":52,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    model = keras.Sequential([\n        layers.Dense(128, activation='relu',input_shape=[X_train.shape[1]]),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(64, activation='relu'),\n       layers.Dense(1)\n      ])\n\n    #optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n    optimizer='rmsprop'\n    \n    model.compile(loss=root_mean_squared_error,\n                optimizer=optimizer,\n                metrics=['mse',])\n    return model","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras_model = build_model()\n\n# EPOCHS = 10\n\n# history = keras_model.fit(\n#   X_train, y_train,\n#   epochs=EPOCHS,\n#     batch_size=100,\n#     validation_data=(X_val, y_val),\n#     #callbacks=[callbacks],\n#   )","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Model accuracy')\n# plt.ylabel('loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# # plt.ylim(bottom=10, top=12)\n# # plt.xlim(left=1100, right = 1200)\n# plt.grid()\n# plt.show()","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Preprocessing of test data, fit model\n# preds_test = keras_model.predict(X_test)\n\n# # The prediction is of type numpy.ndarray\n# preds_list = preds_test.tolist()\n\n# # Extract the prediction and put it in a list\n# prediction = []\n# for item in preds_list:\n#     prediction.append(item[0])\n\n# prediction = np.clip(prediction, 0, 20)","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model = XGBRegressor(\n#     max_depth=8,\n#     n_estimators=1000,\n#     min_child_weight=300, \n#     colsample_bytree=0.8, \n#     subsample=0.8, \n#     eta=0.3,    \n#     seed=42)\n","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n# xgb_model.fit(\n#     X_train, \n#     y_train, \n#     eval_metric=\"rmse\", \n#     eval_set=[(X_train, y_train), (X_val, y_val)], \n#     verbose=True, \n#     early_stopping_rounds = 3)\n\n# time.time() -start","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = xgb_model.predict(X_test).clip(0, 20)\n\n# prediction = (np.clip(preds, 0, 20)).tolist()","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBoost Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\nparams = {'learning_rate': 0.3,\n          'max_depth': 8,\n          'num_iterations': 200,\n          'min_sum_hessian_in_leaf': 300,\n          'feature_fraction': 0.8,\n          'bagging_fraction' : 0.8,\n          'bagging_freq' : 5,\n          'objective': 'regression',\n          'metric': 'rmse',\n          'boosting': 'dart',\n          'verbose': 1\n        }","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = lgb.train(params, train_data,\n                  valid_sets=[valid_data],\n                  num_boost_round=300,\n                  early_stopping_rounds=15,\n                  valid_names=['valid'])\nscore = lgb_model.best_score['valid']['rmse']","execution_count":61,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","name":"stderr"},{"output_type":"stream","text":"[1]\tvalid's rmse: 5.9831\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n  warnings.warn('Early stopping is not available in dart mode')\n","name":"stderr"},{"output_type":"stream","text":"[2]\tvalid's rmse: 5.81685\n[3]\tvalid's rmse: 5.71605\n[4]\tvalid's rmse: 5.63935\n[5]\tvalid's rmse: 5.59947\n[6]\tvalid's rmse: 5.55569\n[7]\tvalid's rmse: 5.52333\n[8]\tvalid's rmse: 5.5519\n[9]\tvalid's rmse: 5.5242\n[10]\tvalid's rmse: 5.49657\n[11]\tvalid's rmse: 5.49097\n[12]\tvalid's rmse: 5.49924\n[13]\tvalid's rmse: 5.49041\n[14]\tvalid's rmse: 5.47929\n[15]\tvalid's rmse: 5.48177\n[16]\tvalid's rmse: 5.47538\n[17]\tvalid's rmse: 5.46013\n[18]\tvalid's rmse: 5.46307\n[19]\tvalid's rmse: 5.46091\n[20]\tvalid's rmse: 5.46205\n[21]\tvalid's rmse: 5.4646\n[22]\tvalid's rmse: 5.46383\n[23]\tvalid's rmse: 5.46368\n[24]\tvalid's rmse: 5.46365\n[25]\tvalid's rmse: 5.47031\n[26]\tvalid's rmse: 5.46608\n[27]\tvalid's rmse: 5.45922\n[28]\tvalid's rmse: 5.45485\n[29]\tvalid's rmse: 5.45676\n[30]\tvalid's rmse: 5.45629\n[31]\tvalid's rmse: 5.45639\n[32]\tvalid's rmse: 5.45228\n[33]\tvalid's rmse: 5.45782\n[34]\tvalid's rmse: 5.4545\n[35]\tvalid's rmse: 5.45569\n[36]\tvalid's rmse: 5.46117\n[37]\tvalid's rmse: 5.46014\n[38]\tvalid's rmse: 5.46273\n[39]\tvalid's rmse: 5.46476\n[40]\tvalid's rmse: 5.47126\n[41]\tvalid's rmse: 5.47245\n[42]\tvalid's rmse: 5.46698\n[43]\tvalid's rmse: 5.46593\n[44]\tvalid's rmse: 5.46515\n[45]\tvalid's rmse: 5.45949\n[46]\tvalid's rmse: 5.46937\n[47]\tvalid's rmse: 5.46416\n[48]\tvalid's rmse: 5.46072\n[49]\tvalid's rmse: 5.46076\n[50]\tvalid's rmse: 5.46515\n[51]\tvalid's rmse: 5.46111\n[52]\tvalid's rmse: 5.4572\n[53]\tvalid's rmse: 5.46079\n[54]\tvalid's rmse: 5.45555\n[55]\tvalid's rmse: 5.45295\n[56]\tvalid's rmse: 5.45615\n[57]\tvalid's rmse: 5.45782\n[58]\tvalid's rmse: 5.46105\n[59]\tvalid's rmse: 5.46715\n[60]\tvalid's rmse: 5.46008\n[61]\tvalid's rmse: 5.46558\n[62]\tvalid's rmse: 5.46171\n[63]\tvalid's rmse: 5.46413\n[64]\tvalid's rmse: 5.47524\n[65]\tvalid's rmse: 5.47397\n[66]\tvalid's rmse: 5.4608\n[67]\tvalid's rmse: 5.45898\n[68]\tvalid's rmse: 5.45718\n[69]\tvalid's rmse: 5.46119\n[70]\tvalid's rmse: 5.46272\n[71]\tvalid's rmse: 5.46502\n[72]\tvalid's rmse: 5.44966\n[73]\tvalid's rmse: 5.44558\n[74]\tvalid's rmse: 5.44611\n[75]\tvalid's rmse: 5.43877\n[76]\tvalid's rmse: 5.43936\n[77]\tvalid's rmse: 5.43987\n[78]\tvalid's rmse: 5.44527\n[79]\tvalid's rmse: 5.44512\n[80]\tvalid's rmse: 5.4471\n[81]\tvalid's rmse: 5.44787\n[82]\tvalid's rmse: 5.43424\n[83]\tvalid's rmse: 5.4348\n[84]\tvalid's rmse: 5.43581\n[85]\tvalid's rmse: 5.43811\n[86]\tvalid's rmse: 5.44285\n[87]\tvalid's rmse: 5.43137\n[88]\tvalid's rmse: 5.43407\n[89]\tvalid's rmse: 5.43708\n[90]\tvalid's rmse: 5.43911\n[91]\tvalid's rmse: 5.44254\n[92]\tvalid's rmse: 5.43762\n[93]\tvalid's rmse: 5.42756\n[94]\tvalid's rmse: 5.42833\n[95]\tvalid's rmse: 5.43169\n[96]\tvalid's rmse: 5.43481\n[97]\tvalid's rmse: 5.43347\n[98]\tvalid's rmse: 5.4374\n[99]\tvalid's rmse: 5.43257\n[100]\tvalid's rmse: 5.4335\n[101]\tvalid's rmse: 5.43843\n[102]\tvalid's rmse: 5.44068\n[103]\tvalid's rmse: 5.44318\n[104]\tvalid's rmse: 5.43939\n[105]\tvalid's rmse: 5.44417\n[106]\tvalid's rmse: 5.4463\n[107]\tvalid's rmse: 5.44992\n[108]\tvalid's rmse: 5.44544\n[109]\tvalid's rmse: 5.44792\n[110]\tvalid's rmse: 5.44905\n[111]\tvalid's rmse: 5.43595\n[112]\tvalid's rmse: 5.43069\n[113]\tvalid's rmse: 5.42835\n[114]\tvalid's rmse: 5.43094\n[115]\tvalid's rmse: 5.43342\n[116]\tvalid's rmse: 5.43763\n[117]\tvalid's rmse: 5.44085\n[118]\tvalid's rmse: 5.44119\n[119]\tvalid's rmse: 5.44333\n[120]\tvalid's rmse: 5.44353\n[121]\tvalid's rmse: 5.44488\n[122]\tvalid's rmse: 5.44725\n[123]\tvalid's rmse: 5.44283\n[124]\tvalid's rmse: 5.44039\n[125]\tvalid's rmse: 5.44379\n[126]\tvalid's rmse: 5.44437\n[127]\tvalid's rmse: 5.44237\n[128]\tvalid's rmse: 5.44522\n[129]\tvalid's rmse: 5.43566\n[130]\tvalid's rmse: 5.43638\n[131]\tvalid's rmse: 5.43844\n[132]\tvalid's rmse: 5.43101\n[133]\tvalid's rmse: 5.42415\n[134]\tvalid's rmse: 5.42522\n[135]\tvalid's rmse: 5.42833\n[136]\tvalid's rmse: 5.43176\n[137]\tvalid's rmse: 5.4344\n[138]\tvalid's rmse: 5.4262\n[139]\tvalid's rmse: 5.42874\n[140]\tvalid's rmse: 5.4303\n[141]\tvalid's rmse: 5.43058\n[142]\tvalid's rmse: 5.43204\n[143]\tvalid's rmse: 5.43247\n[144]\tvalid's rmse: 5.43287\n[145]\tvalid's rmse: 5.42951\n[146]\tvalid's rmse: 5.43169\n[147]\tvalid's rmse: 5.43192\n[148]\tvalid's rmse: 5.43684\n[149]\tvalid's rmse: 5.43636\n[150]\tvalid's rmse: 5.43966\n[151]\tvalid's rmse: 5.44184\n[152]\tvalid's rmse: 5.44432\n[153]\tvalid's rmse: 5.44405\n[154]\tvalid's rmse: 5.445\n[155]\tvalid's rmse: 5.4344\n[156]\tvalid's rmse: 5.43619\n[157]\tvalid's rmse: 5.43611\n[158]\tvalid's rmse: 5.43709\n[159]\tvalid's rmse: 5.43698\n[160]\tvalid's rmse: 5.43759\n[161]\tvalid's rmse: 5.43722\n[162]\tvalid's rmse: 5.43912\n[163]\tvalid's rmse: 5.43416\n[164]\tvalid's rmse: 5.43561\n[165]\tvalid's rmse: 5.43355\n[166]\tvalid's rmse: 5.42938\n[167]\tvalid's rmse: 5.42957\n[168]\tvalid's rmse: 5.42972\n[169]\tvalid's rmse: 5.43022\n[170]\tvalid's rmse: 5.43088\n[171]\tvalid's rmse: 5.42385\n[172]\tvalid's rmse: 5.4255\n[173]\tvalid's rmse: 5.42059\n[174]\tvalid's rmse: 5.40958\n[175]\tvalid's rmse: 5.40755\n[176]\tvalid's rmse: 5.40868\n[177]\tvalid's rmse: 5.41049\n[178]\tvalid's rmse: 5.4124\n[179]\tvalid's rmse: 5.40883\n[180]\tvalid's rmse: 5.41036\n[181]\tvalid's rmse: 5.40331\n[182]\tvalid's rmse: 5.39699\n[183]\tvalid's rmse: 5.39986\n[184]\tvalid's rmse: 5.4\n[185]\tvalid's rmse: 5.40052\n[186]\tvalid's rmse: 5.40494\n[187]\tvalid's rmse: 5.40761\n[188]\tvalid's rmse: 5.40962\n[189]\tvalid's rmse: 5.41171\n[190]\tvalid's rmse: 5.41284\n[191]\tvalid's rmse: 5.40696\n[192]\tvalid's rmse: 5.4062\n[193]\tvalid's rmse: 5.40807\n[194]\tvalid's rmse: 5.40793\n[195]\tvalid's rmse: 5.40816\n[196]\tvalid's rmse: 5.41022\n[197]\tvalid's rmse: 5.41205\n[198]\tvalid's rmse: 5.41309\n[199]\tvalid's rmse: 5.41191\n[200]\tvalid's rmse: 5.41338\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from catboost import CatBoostRegressor\n\n# X_train = f2s(X_train)\n# X_val = f2s(X_val)\n# y_train = f2s(y_train)\n# y_val = f2s(y_val)\n\n\n","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat_features = list(range(0, X_train.shape[1]))\n\n# model = CatBoostRegressor(iterations=15, \n#                           depth=2, \n#                           learning_rate=1, \n#                           loss_function='RMSE')\n# model.fit(\n#     X_train, y_train,\n#     eval_set=(X_val, y_val),\n#     cat_features=cat_features,\n#     verbose = 3,\n# )","execution_count":63,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = all_data.loc[(dates <  34)].drop(to_drop_cols, axis=1)\n# X_test =  all_data.loc[dates == 34].drop(to_drop_cols, axis=1)\n\n# y_train = all_data.loc[(dates <  34), 'target'].values\n\n# train_data = lgb.Dataset(X_train, label=y_train)\n","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb_model = lgb.cv(params,\n#                 train_data,\n#                 num_boost_round=300,\n#                 early_stopping_rounds=50,\n#                 stratified=False,\n#                 verbose_eval=True, \n#                 show_stdv=True)\n\n","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = lgb_model.predict(X_test).clip(0, 20)\n\nprediction = (np.clip(preds, 0, 20)).tolist()","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'ID': sales_index,\n                       'item_cnt_month': prediction})\n","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output.to_csv('KerasModel.csv', index=False)\n# output.to_csv('XGBModel.csv', index=False)\noutput.to_csv('LGBModel.csv', index=False)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from xgboost import plot_importance\n\n# def plot_features(booster, figsize):    \n#     fig, ax = plt.subplots(1,1,figsize=figsize)\n#     return plot_importance(booster=booster, ax=ax, importance_type='weight')\n\n# plot_features(xgb_model, (10,14))","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_gain = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='gain'), index=['gain'])\n# df_weight = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='weight'), index=['weight'])\n# df_cover = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='cover'), index=['cover'])\n# # df_total_gain = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='total_gain'), index=['total_gain'])\n# # df_total_cover = pd.DataFrame(xgb_model.get_booster().get_score(fmap='', importance_type='total_cover'), index=['total_cover'])\n\n# df_importance = df_gain.append(df_weight, ignore_index=False, sort=False)\n# df_importance = df_importance.append(df_cover, ignore_index=False, sort=False)\n# # df_importance = df_importance.append(df_total_gain, ignore_index=False, sort=False)\n# # df_importance = df_importance.append(df_total_cover, ignore_index=True, sort=False)\n\n# df_importance.iloc[0] = df_importance.iloc[0]/df_importance.iloc[0].max()\n# df_importance.iloc[1] = df_importance.iloc[1]/df_importance.iloc[1].max()\n# df_importance.iloc[2] = df_importance.iloc[2]/df_importance.iloc[2].max()\n# df_T=df_importance.T\n# df_T.to_csv('importance.csv')\n\n# df_T.plot.bar(figsize=(15,20))\n\n# df_T.tail(5)\n","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# file1 = 'KerasModel.csv'\n# k1 =pd.read_csv(file1)\n# file2 = 'XGBModel.csv'\n# k2 =pd.read_csv(file2)\n# file3 = 'LGBModel.csv'\n# k3 =pd.read_csv(file3)\n\n# k3['item_cnt_month'] = (k1['item_cnt_month'] + k2['item_cnt_month'] + k3['item_cnt_month'])/3\n# k3.to_csv('All_Models_mean.csv', index=False)","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}